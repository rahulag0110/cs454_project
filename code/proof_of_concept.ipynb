{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\mfaizanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Iteration 0/1500 - Loss: 0.470703125\n",
      "Iteration 100/1500 - Loss: -3.1037638187408447\n",
      "Iteration 200/1500 - Loss: -3.272459030151367\n",
      "Iteration 300/1500 - Loss: -3.3146493434906006\n",
      "Iteration 400/1500 - Loss: -3.3419196605682373\n",
      "Iteration 500/1500 - Loss: -3.3389148712158203\n",
      "Iteration 600/1500 - Loss: -3.3524906635284424\n",
      "Iteration 700/1500 - Loss: -3.3675856590270996\n",
      "Iteration 800/1500 - Loss: -3.362954616546631\n",
      "Iteration 900/1500 - Loss: -3.366569995880127\n",
      "Iteration 1000/1500 - Loss: -3.3886606693267822\n",
      "Iteration 1100/1500 - Loss: -3.372663974761963\n",
      "Iteration 1200/1500 - Loss: -3.3955347537994385\n",
      "Iteration 1300/1500 - Loss: -3.2939345836639404\n",
      "Iteration 1400/1500 - Loss: -3.4009644985198975\n",
      "Iteration 1499/1500 - Loss: -3.39396333694458\n",
      "Poisoned image saved as poisoned_image_refined.png\n"
     ]
    }
   ],
   "source": [
    "# old works\n",
    "import torch\n",
    "import lpips\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip import load\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load LPIPS loss model\n",
    "lpips_loss = lpips.LPIPS(net=\"vgg\").to(device)\n",
    "\n",
    "# Load images\n",
    "def load_image(filepath):\n",
    "    img = Image.open(filepath).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    return transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "cat_image = load_image(\"cat.webp\")\n",
    "dog_image = load_image(\"dog.webp\")\n",
    "\n",
    "# Use a copy of the cat image as the starting point for poisoning\n",
    "poisoned_image = cat_image.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Define optimizer and hyperparameters\n",
    "optimizer = torch.optim.Adam([poisoned_image], lr=0.01)\n",
    "iterations = 1500  # Increased for more refinement\n",
    "lpips_weight = 1.0  # Weight for perceptual similarity loss\n",
    "clip_weight = 10.0  # Weight for CLIP similarity to \"a dog\"\n",
    "\n",
    "# Target text embeddings\n",
    "text_cat = clip_model.encode_text(clip.tokenize([\"a cat\"]).to(device)).detach()\n",
    "text_dog = clip_model.encode_text(clip.tokenize([\"a dog\"]).to(device)).detach()\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate LPIPS loss (perceptual similarity to cat)\n",
    "    lpips_value = lpips_loss(poisoned_image, cat_image)\n",
    "\n",
    "    # Calculate CLIP similarity scores\n",
    "    image_features = clip_model.encode_image(poisoned_image)\n",
    "    clip_sim_to_dog = torch.cosine_similarity(image_features, text_dog, dim=1)\n",
    "    clip_sim_to_cat = torch.cosine_similarity(image_features, text_cat, dim=1)\n",
    "\n",
    "    # Loss: increase similarity to \"a dog\" while staying perceptually close to \"a cat\"\n",
    "    loss = (\n",
    "        lpips_weight * lpips_value.mean()\n",
    "        - clip_weight * clip_sim_to_dog.mean()\n",
    "        + clip_weight * clip_sim_to_cat.mean()\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clamp pixel values to valid range [0, 1]\n",
    "    poisoned_image.data = torch.clamp(poisoned_image.data, 0, 1)\n",
    "\n",
    "    if i % 100 == 0 or i == iterations - 1:\n",
    "        print(f\"Iteration {i}/{iterations} - Loss: {loss.item()}\")\n",
    "\n",
    "# Save the final poisoned image\n",
    "poisoned_image_tensor = poisoned_image.detach().squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "poisoned_image_pil = Image.fromarray((poisoned_image_tensor * 255).astype(\"uint8\"))\n",
    "poisoned_image_pil.save(\"poisoned_image_refined.png\")\n",
    "print(\"Poisoned image saved as poisoned_image_refined.png\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\mfaizanz\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\lpips\\weights\\v0.1\\vgg.pth\n",
      "Iteration 0/1500 - Loss: 0.59765625\n",
      "Iteration 100/1500 - Loss: -2.829263210296631\n",
      "Iteration 200/1500 - Loss: -3.087207794189453\n",
      "Iteration 300/1500 - Loss: -3.1105921268463135\n",
      "Iteration 400/1500 - Loss: -3.1543965339660645\n",
      "Iteration 500/1500 - Loss: -3.1767537593841553\n",
      "Iteration 600/1500 - Loss: -3.217648506164551\n",
      "Iteration 700/1500 - Loss: -3.2401742935180664\n",
      "Iteration 800/1500 - Loss: -3.239015817642212\n",
      "Iteration 900/1500 - Loss: -3.2649776935577393\n",
      "Iteration 1000/1500 - Loss: -3.24422550201416\n",
      "Iteration 1100/1500 - Loss: -3.2723782062530518\n",
      "Iteration 1200/1500 - Loss: -3.2668821811676025\n",
      "Iteration 1300/1500 - Loss: -3.2586886882781982\n",
      "Iteration 1400/1500 - Loss: -3.2438607215881348\n",
      "Iteration 1499/1500 - Loss: -3.2828564643859863\n",
      "Poisoned image saved as dog_poisoned_image.png\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import lpips\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip import load\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load LPIPS loss model\n",
    "lpips_loss = lpips.LPIPS(net=\"vgg\").to(device)\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_image(filepath):\n",
    "    img = Image.open(filepath).convert(\"RGB\")\n",
    "    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n",
    "    return transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# Load images\n",
    "cat_image = load_image(\"cat.webp\")\n",
    "dog_image = load_image(\"dog.webp\")\n",
    "\n",
    "# Use a copy of the dog image as the starting point for poisoning\n",
    "poisoned_image = dog_image.clone().detach().requires_grad_(True)\n",
    "\n",
    "# Define optimizer and hyperparameters\n",
    "optimizer = torch.optim.Adam([poisoned_image], lr=0.01)\n",
    "iterations = 1500  # Number of optimization steps\n",
    "lpips_weight = 1.0  # Weight for perceptual similarity loss\n",
    "clip_weight = 10.0  # Weight for CLIP similarity to \"a cat\"\n",
    "\n",
    "# Target text embeddings\n",
    "text_cat = clip_model.encode_text(clip.tokenize([\"a cat\"]).to(device)).detach()\n",
    "text_dog = clip_model.encode_text(clip.tokenize([\"a dog\"]).to(device)).detach()\n",
    "\n",
    "# Optimization loop\n",
    "for i in range(iterations):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Calculate LPIPS loss (perceptual similarity to dog)\n",
    "    lpips_value = lpips_loss(poisoned_image, dog_image)\n",
    "\n",
    "    # Calculate CLIP similarity scores\n",
    "    image_features = clip_model.encode_image(poisoned_image)\n",
    "    clip_sim_to_cat = torch.cosine_similarity(image_features, text_cat, dim=1)\n",
    "    clip_sim_to_dog = torch.cosine_similarity(image_features, text_dog, dim=1)\n",
    "\n",
    "    # Loss: increase similarity to \"a cat\" while staying perceptually close to \"a dog\"\n",
    "    loss = (\n",
    "        lpips_weight * lpips_value.mean()\n",
    "        - clip_weight * clip_sim_to_cat.mean()\n",
    "        + clip_weight * clip_sim_to_dog.mean()\n",
    "    )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Clamp pixel values to valid range [0, 1]\n",
    "    poisoned_image.data = torch.clamp(poisoned_image.data, 0, 1)\n",
    "\n",
    "    # Print progress\n",
    "    if i % 100 == 0 or i == iterations - 1:\n",
    "        print(f\"Iteration {i}/{iterations} - Loss: {loss.item()}\")\n",
    "\n",
    "# Save the final poisoned image\n",
    "poisoned_image_tensor = poisoned_image.detach().squeeze().permute(1, 2, 0).cpu().numpy()\n",
    "poisoned_image_pil = Image.fromarray((poisoned_image_tensor * 255).astype(\"uint8\"))\n",
    "poisoned_image_pil.save(\"dog_poisoned_image.png\")\n",
    "print(\"Poisoned image saved as dog_poisoned_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity scores for cat.webp:\n",
      "  a cat: 0.2534\n",
      "  a dog: 0.2063\n",
      "  a car: 0.1832\n",
      "  a tree: 0.1753\n",
      "  a house: 0.1829\n",
      "\n",
      "Similarity scores for dog.webp:\n",
      "  a cat: 0.1954\n",
      "  a dog: 0.2588\n",
      "  a car: 0.1805\n",
      "  a tree: 0.1611\n",
      "  a house: 0.1715\n",
      "\n",
      "Similarity scores for poisoned_image_refined.png:\n",
      "  a cat: 0.1962\n",
      "  a dog: 0.2600\n",
      "  a car: 0.1809\n",
      "  a tree: 0.1617\n",
      "  a house: 0.1705\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load the CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# Load the images and preprocess them\n",
    "cat_image = preprocess(Image.open(\"cat.webp\").convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "dog_image = preprocess(Image.open(\"dog.webp\").convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "poisoned_image = preprocess(Image.open(\"dog-nightshade-intensity-DEFAULT-V1.png\").convert(\"RGB\")).unsqueeze(0).to(device)\n",
    "\n",
    "# Define a list of possible text labels\n",
    "text_labels = [\"a cat\", \"a dog\", \"a car\", \"a tree\", \"a house\"]\n",
    "text_inputs = clip.tokenize(text_labels).to(device)\n",
    "\n",
    "# Encode the images and text using CLIP\n",
    "with torch.no_grad():\n",
    "    cat_features = model.encode_image(cat_image)\n",
    "    dog_features = model.encode_image(dog_image)\n",
    "    poisoned_features = model.encode_image(poisoned_image)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# Normalize the features for cosine similarity\n",
    "cat_features /= cat_features.norm(dim=-1, keepdim=True)\n",
    "dog_features /= dog_features.norm(dim=-1, keepdim=True)\n",
    "poisoned_features /= poisoned_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# Compute similarity scores\n",
    "cat_similarity = (cat_features @ text_features.T).squeeze()\n",
    "dog_similarity = (dog_features @ text_features.T).squeeze()\n",
    "poisoned_similarity = (poisoned_features @ text_features.T).squeeze()\n",
    "\n",
    "# Print similarity scores for each label\n",
    "print(\"Similarity scores for cat.webp:\")\n",
    "for i, label in enumerate(text_labels):\n",
    "    print(f\"  {label}: {cat_similarity[i].item():.4f}\")\n",
    "\n",
    "print(\"\\nSimilarity scores for dog.webp:\")\n",
    "for i, label in enumerate(text_labels):\n",
    "    print(f\"  {label}: {dog_similarity[i].item():.4f}\")\n",
    "\n",
    "print(\"\\nSimilarity scores for poisoned_image_refined.png:\")\n",
    "for i, label in enumerate(text_labels):\n",
    "    print(f\"  {label}: {poisoned_similarity[i].item():.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
